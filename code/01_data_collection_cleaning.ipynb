{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection\n",
    "\n",
    "**Note:** The code discussed below will be in the data_colection.py located in the script directory. The data collection step is broken down into two parts: First and Second Request. \n",
    "\n",
    "---\n",
    "### First Request:\n",
    "\n",
    "The code below was used to conduct the First Request part. Since the project's goal is to create an HBO Max Recommender System, I only collected shows/movies from that platform. Additionally, for project simplicity, I only isolated shows/movies from the U.S. library. To start, I conducted a single pull using the `initial_data` function. The output dictionary contains a key called total_count, which was used as a guiding parameter to dictate when the while loop will stop. This is embedded in the `hbo_content_list` function to collected the remainder data. The collected data are then arranged into a dataframe using the `json_df_content` function and saved as df_1. \n",
    "\n",
    "A problem observed from the scraper is that it throws an error if the status code is not equal to 200. Therefore a try and except statement is integrated inside the `hbo_content_list` function. Additionally, I picked each collected data into a .txt file to prevent data loss. I also added print statements to track page count and the number of data collected, making it easy to resume the data collection if the cycle breaks. Lastly, I throttled each request for 5 seconds to comply with politeness and rate-limiting policies. This number chosen was based on a trial and error process.\n",
    "\n",
    "```\n",
    "def initial_data():     \n",
    "    data = just_watch.search_for_item(providers=['hbm'], page=1)\n",
    "    return [data], data['total_results']\n",
    "\n",
    "def hbo_content_list():\n",
    "    data_list, total_size = intial_data() \n",
    "    size = len(data_list[0]['items']) \n",
    "    page_num = 2 \n",
    "    while size < total_size:\n",
    "        try: \n",
    "            data = just_watch.search_for_item(providers=['hbm'], page=page_num)\n",
    "            data_list.append(data)\n",
    "            size += len(data['items'])\n",
    "            page_num += 1 \n",
    "            if size % 30 == 0: \n",
    "                print(\"Number of data pulled:\")\n",
    "                print(size)\n",
    "                print(\"Page Number:\")\n",
    "                print(page_num - 1)\n",
    "            with open('initial_data.txt', 'wb') as output:\n",
    "                pickle.dump(data_list, output)\n",
    "            time.sleep(5)\n",
    "        except:\n",
    "            pass\n",
    "    return data_list\n",
    "\n",
    "def json_df_content():\n",
    "    content = []\n",
    "    data_list = hbo_content_list()\n",
    "    for items in data_list:\n",
    "        for item in items['items']:\n",
    "            show = {}\n",
    "            show['id'] = item['id']\n",
    "            show['title'] = item['title']\n",
    "            show['type'] = item['object_type']\n",
    "            content.append(show)\n",
    "    return pd.DataFrame(content)\n",
    "\n",
    "df_1 = json_df_content()\n",
    "df_1.to_csv('df_1.csv')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Second Request:\n",
    "\n",
    "The code below will conduct the Second Request of the data collection step, which will collect the granular details needed for the EDA and modeling/recommender process. The first function, `add_info`, will take df_1 as input and parse each title's id and type through the JustWatchAPI get_title() method. This method will extract the additional information needed and return it as a JSON file. These are then arranged into a dataframe using the `json_df_add` function with the following columns (id, plot, MPAA/TV rating, genre, popularity score, IMDB rating, and TMDB rating). Like the code block above, the same precautions are used for the second part. Each loops are throttled by 5 seconds. I also pickled collected data to .txt files and added print statements to track the collection process. The second part's output dataframe is concatenated to df_1, creating a final dataframe consisting of 10 columns and 1980 rows and is saved into a CSV file (hbo_data.csv). \n",
    "\n",
    "```\n",
    "df_1 = json_df_content()\n",
    "df_1.to_csv('df_1.csv')\n",
    "\n",
    "def add_info(df):\n",
    "    content = []\n",
    "    for i in df.index:\n",
    "        show = just_watch.get_title(title_id = df.loc[i, 'id'], content_type= df.loc[i, 'type'])\n",
    "        content.append(show)\n",
    "        with open('raw_data.txt', 'wb') as output:\n",
    "            pickle.dump(content, output)\n",
    "        print(\"Number of data pulled:\")\n",
    "        print(len(content))\n",
    "        time.sleep(5)\n",
    "    return content\n",
    "    \n",
    "def json_df_add():\n",
    "    content = []\n",
    "    raw_data = add_info(df_1)\n",
    "    for data in raw_data:\n",
    "        show_info = {}\n",
    "        show_info['year'] = data.get('original_release_year')\n",
    "        show_info['plot'] = data.get('short_description')\n",
    "        show_info['genre'] = data.get('genre_ids')\n",
    "        show_info['rating'] = data.get('age_certification')\n",
    "        if data.get('scoring') == None:\n",
    "                show_info['avg_rating'] = None \n",
    "        else:\n",
    "            for score in data.get('scoring'):\n",
    "                if score['provider_type'] == 'imdb:score':\n",
    "                    show_info['avg_rating'] = score['value']\n",
    "                elif score['provider_type'] == 'tmdb:score':\n",
    "                    show_info['avg_rating'] = score['value']\n",
    "                if score['provider_type'] == 'tmdb:popularity':\n",
    "                    show_info['popularity_score'] = score['value']\n",
    "        content.append(show_info)\n",
    "    return pd.DataFrame(content)\n",
    "\n",
    "df_2 = json_df_add()\n",
    "df_2.to_csv('df_2.csv')\n",
    "hbo_data = pd.concat([df_1, add_info_2], axis=1)\n",
    "hbo_data.to_csv('hbo_data.csv') \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Data Cleaning \n",
    "\n",
    "Approximately 20% of the data is missing due in part of the API not possessing the information. Therefore, missing values are manually imputed using information from the IMDB website. Ideally, a process similar to the ones above would have been preferable. However, IMDB requires an API key to access its information. Due to this project's timeline, it would not be feasible to wait for a response on their end. Additionally, their scrapper would require re-doing the entire process since each shows and movies will have a different id. The combined data frame was run through `convert_genre()` and `fix_rating()` functions to change the genre ids to their corresponding term and remove the spaces at the end of the MPAA/TV ratings. \n",
    "\n",
    "**Note:** `convert_genre()` and `fix_rating()` is located inside the `functions.py` in the script directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import script.functions as func\n",
    "from justwatch import JustWatch\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "df = pd.read_csv('../data/hbo_data.csv')\n",
    "df = func.convert_genre(df)\n",
    "df = func.fix_rating(df)\n",
    "df.to_csv('../data/final_hbo_data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection\n",
    "\n",
    "**Note:** The code discussed below will be in the data_colection.py located in the script directory.\n",
    "\n",
    "---\n",
    "### First Request:\n",
    "\n",
    "The code below was used to make the first request from the JustWatch.com API. Since the project's goal is to create an HBO Max Recommender System, I only collected shows/movies from that platform. Additionally, for project simplicity, I only isolated shows/movies from the U.S. library. To start, I conducted a single pull using the `initial_data()` function. The output dictionary will act as a guide for the remainder of the data collection. For this project, I used the value from the `total_result` key as the entire HBO Max library's total count, enabling the while loop to submit requests until the defined count is reached.\n",
    "\n",
    "A problem observed from the scraper is that it throws an error if the status code is not 200. Therefore a try and except code is integrated inside the `hbo_content_list()` function. Moreover, each pulled request is pickled into a txt file stored in the data directory to prevent data loss. Print statements are also embedded in the function to check for pull status, making it easier to continue if the cycle breaks. Lastly, I throttled each request for 5 seconds to comply with politeness and rate-limiting policies. This number was chosen based on a trial and error process. The final function arranges the pulled data into a data frame, while isolating the necessary information.\n",
    "\n",
    "```\n",
    "def initial_data():     \n",
    "    data = just_watch.search_for_item(providers=['hbm'], page=1)\n",
    "    return [data], data['total_results']\n",
    "\n",
    "def hbo_content_list():\n",
    "    data_list, total_size = intial_data() \n",
    "    size = len(data_list[0]['items']) \n",
    "    page_num = 2 \n",
    "    while size < total_size:\n",
    "        try: \n",
    "            data = just_watch.search_for_item(providers=['hbm'], page=page_num)\n",
    "            data_list.append(data)\n",
    "            size += len(data['items'])\n",
    "            page_num += 1 \n",
    "            if size % 30 == 0: \n",
    "                print(\"Number of data pulled:\")\n",
    "                print(size)\n",
    "                print(\"Page Number:\")\n",
    "                print(page_num - 1)\n",
    "            with open('initial_data.txt', 'wb') as output:\n",
    "                pickle.dump(data_list, output)\n",
    "            time.sleep(5)\n",
    "        except:\n",
    "            pass\n",
    "    return data_list\n",
    "\n",
    "def json_df_content():\n",
    "    content = []\n",
    "    data_list = hbo_content_list()\n",
    "    for items in data_list:\n",
    "        for item in items['items']:\n",
    "            show = {}\n",
    "            show['id'] = item['id']\n",
    "            show['title'] = item['title']\n",
    "            show['type'] = item['object_type']\n",
    "            content.append(show)\n",
    "    return pd.DataFrame(content)\n",
    "\n",
    "df_1 = json_df_content()\n",
    "df_1.to_csv('df_1.csv')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Second Request:\n",
    "\n",
    "The code below will collect the granular details needed for the EDA and Modeling/Recommender process. The first function, `add_info()` will take in the final data frame output from the code above. It would then parse each show/movie id through the JustWatchAPI `get_title()` method. This method will extract the additional information and return it as a JSON file. Like the above code block, each loop is throttled by 5 seconds, and extracted data are pickled into a .txt file. The output list of dictionaries (JSON files) will be used as input for the `json_df()` function. This code will convert the data into a data frame extracting only the needed information. Lastly, it is concatenated to the output data frame from the above code block and save into a CSV file.\n",
    "\n",
    "```\n",
    "df_1 = json_df_content()\n",
    "df_1.to_csv('df_1.csv')\n",
    "\n",
    "def add_info(df):\n",
    "    content = []\n",
    "    for i in df.index:\n",
    "        show = just_watch.get_title(title_id = df.loc[i, 'id'], content_type= df.loc[i, 'type'])\n",
    "        content.append(show)\n",
    "        with open('raw_data.txt', 'wb') as output:\n",
    "            pickle.dump(content, output)\n",
    "        print(\"Number of data pulled:\")\n",
    "        print(len(content))\n",
    "        time.sleep(5)\n",
    "    return content\n",
    "    \n",
    "def json_df_add():\n",
    "    content = []\n",
    "    raw_data = add_info(df_1)\n",
    "    for data in raw_data:\n",
    "        show_info = {}\n",
    "        show_info['year'] = data.get('original_release_year')\n",
    "        show_info['plot'] = data.get('short_description')\n",
    "        show_info['genre'] = data.get('genre_ids')\n",
    "        show_info['rating'] = data.get('age_certification')\n",
    "        if data.get('scoring') == None:\n",
    "                show_info['avg_rating'] = None \n",
    "        else:\n",
    "            for score in data.get('scoring'):\n",
    "                if score['provider_type'] == 'imdb:score':\n",
    "                    show_info['avg_rating'] = score['value']\n",
    "                elif score['provider_type'] == 'tmdb:score':\n",
    "                    show_info['avg_rating'] = score['value']\n",
    "                if score['provider_type'] == 'tmdb:popularity':\n",
    "                    show_info['popularity_score'] = score['value']\n",
    "        content.append(show_info)\n",
    "    return pd.DataFrame(content)\n",
    "\n",
    "df_2 = json_df_add()\n",
    "df_2.to_csv('df_2.csv')\n",
    "hbo_data = pd.concat([df_1, add_info_2], axis=1)\n",
    "hbo_data.to_csv('hbo_data.csv') \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Data Cleaning \n",
    "\n",
    "The final data frame contains 10 columns and 1980 rows. Approximately 20% of the data is missing due in part of the API not possessing the information. Therefore, missing values are manually imputed using information from the IMDB website. Ideally, a process similar to the one above would have been preferable. However, IMDB requires an API key to access its information. Due to this project's timeline, it would not be feasible to wait for a response on their end. Additionally, their scrapper would require re-doing the entire process since each shows and movies will have a different id. The combined data frame was run through `convert_genre()` and `fix_rating()` functions to change the genre ids to their corresponding term and remove the spaces at the end of the MPAA/TV ratings. \n",
    "\n",
    "**Note:** `convert_genre()` and `fix_rating()` is located inside the `functions.py` in the script directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import script.functions as func\n",
    "from justwatch import JustWatch\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "df = pd.read_csv('../data/hbo_data.csv')\n",
    "df = func.convert_genre(df)\n",
    "df = func.fix_rating(df)\n",
    "df.to_csv('../data/final_hbo_data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
